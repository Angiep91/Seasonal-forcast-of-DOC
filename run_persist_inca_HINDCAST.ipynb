{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08321,
     "end_time": "2022-03-14T17:49:44.888825",
     "exception": false,
     "start_time": "2022-03-14T17:49:44.805615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![logo](./img/LogoLine_horizon_C3S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081751,
     "end_time": "2022-03-14T17:49:45.053733",
     "exception": false,
     "start_time": "2022-03-14T17:49:44.971982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082814,
     "end_time": "2022-03-14T17:49:45.21891",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.136096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PREPARE FILES AND RUN PERSIST and INCA FOR HINDCAST PERIOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080985,
     "end_time": "2022-03-14T17:49:45.381362",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.300377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080595,
     "end_time": "2022-03-14T17:49:45.706112",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.625517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Create a .bat file to run PERSIST\n",
    "2. Run PERSIST for all hindcast files\n",
    "3. Correct HER outputs from PERSIST\n",
    "4. Extract SOC time series from INCA forced with ERA5 to reset initial conditions of SOC in hindcast runs (when required)\n",
    "5. Run INCA and process outputs\n",
    "6. Process INCA outputs from ERA5  (when required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081079,
     "end_time": "2022-03-14T17:49:46.683509",
     "exception": false,
     "start_time": "2022-03-14T17:49:46.60243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 2.519057,
     "end_time": "2022-03-14T17:52:09.394032",
     "exception": false,
     "start_time": "2022-03-14T17:52:06.874975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Miscellaneous operating system interfaces\n",
    "import os\n",
    "\n",
    "# Libraries for working with multi-dimensional arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# Libraries for plotting and geospatial data visualisation\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# To work with data labels in dictionary format\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Date and time related libraries\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from calendar import monthrange\n",
    "import datetime\n",
    "\n",
    "# Copy files to another path directory\n",
    "import shutil\n",
    "\n",
    "#Time\n",
    "import time\n",
    "\n",
    "#to run models\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/'\n",
    "os.chdir(path) #always run this line before runing a new test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create .bat file with all hindcast .par and .dat files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files extracted from \"hindcast_bc_persist_input\" folder for .dat files\n",
    "#The best optimized .par files (october 2024) were used to create .par files (location: hindcast_run_model) \n",
    "\n",
    "# Open the .bat file in write mode\n",
    "with open(f'{path}hindcast_persist_files/bat_persist.bat', 'w') as bat_file:\n",
    "    # Loop through members, years, and months\n",
    "    for member in range(0, 25):  # member from 0 to 24\n",
    "        for y in range(0, 24):  # year from 0 to 23\n",
    "            for m in range(0, 12):  # month from 0 to 11\n",
    "                \n",
    "                month = 1 + m\n",
    "                year = 1993 + y\n",
    "                start_year = year - 5  # IMPORTANT: Change the number of warmup years if required !!!!!!!!!!!!\n",
    "                            \n",
    "                # Update PERSiST parameters\n",
    "                with open(f'{path}hindcast_bc_persist_input/hind_merged__member{member}_year{y}_month{m}.csv', 'r') as file:\n",
    "                    lines_dat = file.readlines()   # take Steps number from .dat file\n",
    "                    \n",
    "                with open(f'{path}hindcast_run_model/persist_par.par', 'r') as file:\n",
    "                    lines = file.readlines()\n",
    "\n",
    "                lines[0] = lines_dat[0]          # update Steps number\n",
    "                lines[1] = f'1/{month}/{start_year}\\n' # update Date\n",
    "\n",
    "                # Save .par files\n",
    "                par_filename = f'persist_par_member{member}_year{y}_month{m}.par'\n",
    "                with open(f'{path}hindcast_persist_files/{par_filename}', 'w') as file:\n",
    "                    file.writelines(lines)\n",
    "                \n",
    "                # Remove quotes from CSV content before saving as .dat\n",
    "                dat_filename = f'persist_dat_member{member}_year{y}_month{m}.dat'\n",
    "                \n",
    "                with open(f'{path}hindcast_bc_persist_input/hind_merged__member{member}_year{y}_month{m}.csv', 'r') as infile:\n",
    "                    lines = infile.readlines()\n",
    "                \n",
    "                with open(f'{path}hindcast_persist_files/{dat_filename}', 'w') as outfile:\n",
    "                    for line in lines:\n",
    "                        # Remove quotes from the line\n",
    "                        clean_line = line.replace('\"', '')\n",
    "                        outfile.write(clean_line)\n",
    "                \n",
    "                # Write the command to the .bat file\n",
    "                cmd = f'persist_cmd_0.exe -par {par_filename} -dat {dat_filename} -inca cmd_member{member}_year{y}_month{m} -out output_member{member}_year{y}_month{m} -size all\\n'\n",
    "                bat_file.write(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Execute the created .bat file (double click) to run PERSIST for the hindcast .bat file (bat_persist) performed in the \"hindcast_persist_files\" folder. The cmd version of PERSiST, \"persist_cmd_0.exe\", must be present in the same folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recalculate HER manually for Subcatchments C1 and C2 (code developed based on formulas provided by Martyn Futter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Subcatchment C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate HER\n",
    "def calculate_HER_final(snowFiles, runOffFiles, HER_from_persist, output_file):\n",
    "\n",
    "    # Rain and snow melt calculation\n",
    "    rainPlusSnowmelt = [[], [], [], [], []]\n",
    "\n",
    "    for i in range(5):\n",
    "        reach_c1_found = False  # Reset the flag for each file\n",
    "\n",
    "        with open(snowFiles[i], 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "            for line in lines:\n",
    "                if 'C1' in line:\n",
    "                    reach_c1_found = True\n",
    "                    continue\n",
    "                if 'Snowfall,Snow melt,Snow depth,Rain' in line:\n",
    "                    continue\n",
    "                if 'C2' in line:\n",
    "                    break\n",
    "            \n",
    "                columns = line.strip().split(',')\n",
    "                if reach_c1_found and len(columns) == 5:\n",
    "                    snow_melt = float(columns[2].strip())\n",
    "                    rain = float(columns[4].strip())\n",
    "                    rainPlusSnowmelt[i].append(snow_melt + rain)\n",
    "                \n",
    "    rainPlusSnowmelt = np.array(rainPlusSnowmelt)\n",
    "\n",
    "    # Runoff extraction\n",
    "    runOff = [[], [], [], [], []]\n",
    "\n",
    "    for i in range(5):\n",
    "        reach_c1_found = False  # Reset the flag for each file\n",
    "\n",
    "        with open(runOffFiles[i], 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "            for line in lines:\n",
    "                if 'C1' in line:\n",
    "                    reach_c1_found = True\n",
    "                    continue\n",
    "                if 'Runoff' in line:\n",
    "                    continue\n",
    "                if 'C2' in line:\n",
    "                    break\n",
    "            \n",
    "                columns = line.strip().split(',')\n",
    "                if reach_c1_found and len(columns) == 2:\n",
    "                    runOff[i].append(float(columns[1].strip()))\n",
    "\n",
    "    runOff = np.array(runOff)\n",
    "\n",
    "    # HER calculation\n",
    "    HER = np.zeros([5, runOff.shape[1]])\n",
    "    percValues = [0.0609, 0.2333, 0.3533, 0.3423, 0.0102] #land use percentages for C1\n",
    "    cumRunOff = np.zeros([5, runOff.shape[1]])\n",
    "    cumRunOff[:, -1] = runOff[:, -1]\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in np.arange(runOff.shape[1] - 2, -1, -1):\n",
    "            cumRunOff[i, j] = cumRunOff[i, j + 1] + runOff[i, j] - HER[i, j + 1]\n",
    "            HER[i, j] = np.min([rainPlusSnowmelt[i, j], cumRunOff[i, j]])\n",
    "\n",
    "    HER_partial = np.zeros([5, runOff.shape[1]])\n",
    "    for i in range(5):\n",
    "        HER_partial[i, :] = percValues[i] * HER[i, :]\n",
    "\n",
    "    HER_final = np.sum(HER_partial, axis=0)\n",
    "\n",
    "\n",
    "    # Change the corrected HER values (HER_final) into the HER_from_persist file\n",
    "    df = pd.read_csv(HER_from_persist, header= None, sep=None)  # Use header=None if the file doesn't have a header\n",
    "    # Update the second column (index 1) with HER_final values\n",
    "    df.iloc[:, 1] = HER_final  # Directly assign HER_final values to the second column\n",
    "    # Write the updated DataFrame back to a CSV file\n",
    "    df.to_csv(output_file, index=False, header=False, sep= ' ')  # Use index=False to avoid adding an extra index column\n",
    "\n",
    "    return HER_final\n",
    "\n",
    "\n",
    "#Main loop\n",
    "for member in range(0, 25):  # member from 0 to 24\n",
    "    for y in range(0, 24):    # year from 0 to 23\n",
    "        for m in range(0, 12):  # month from 0 to 11\n",
    "            # Define file paths for each iteration\n",
    "            snow_agr = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Agriculture.csv'\n",
    "            snow_broad_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Broad_leaved_Forest.csv'\n",
    "            snow_conf_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Coniferous_Forest.csv'\n",
    "            snow_small_veg = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Small_NoVegetation.csv'\n",
    "            snow_urb = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Urban.csv'\n",
    "\n",
    "            snowFiles = [snow_agr, snow_broad_forest, snow_conf_forest, snow_small_veg, snow_urb]\n",
    "\n",
    "            runoff_agr = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Agriculture.csv'\n",
    "            runoff_broad_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Broad_leaved_Forest.csv'\n",
    "            runoff_conf_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Coniferous_Forest.csv'\n",
    "            runoff_small_veg = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Small_NoVegetation.csv'\n",
    "            runoff_urb = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Urban.csv'\n",
    "\n",
    "            runOffFiles = [runoff_agr, runoff_broad_forest, runoff_conf_forest, runoff_small_veg, runoff_urb]\n",
    "            \n",
    "            HER_from_persist = f'{path}/hindcast_persist_files/cmd_member{member}_year{y}_month{m} C1.dat'\n",
    "            output_file = f'{path}/hindcast_inca_files/inca_C1_member{member}_year{y}_month{m}.dat'\n",
    "\n",
    "            # Call the HER calculation function with current file paths\n",
    "            HER_final = calculate_HER_final(snowFiles, runOffFiles, HER_from_persist, output_file)\n",
    "\n",
    "            # Optionally\n",
    "            print(f'Processed member={member}, year={y}, month={m}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Subcatchment C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate HER\n",
    "def calculate_HER_final(snowFiles, runOffFiles, HER_from_persist, output_file):\n",
    "\n",
    "    # Rain and snow melt calculation\n",
    "    rainPlusSnowmelt = [[], [], [], [], []]\n",
    "\n",
    "    for i in range(5):\n",
    "        reach_c2_found = False  # Reset the flag for each file\n",
    "\n",
    "        with open(snowFiles[i], 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "            for line in lines:\n",
    "                if 'C2' in line:\n",
    "                    reach_c2_found = True\n",
    "                    continue\n",
    "                if 'Snowfall,Snow melt,Snow depth,Rain' in line:\n",
    "                    continue\n",
    "                \n",
    "                columns = line.strip().split(',')\n",
    "                \n",
    "                if reach_c2_found and len(columns) == 5:\n",
    "                    snow_melt = float(columns[2].strip())\n",
    "                    rain = float(columns[4].strip())\n",
    "                    rainPlusSnowmelt[i].append(snow_melt + rain)\n",
    "                \n",
    "    rainPlusSnowmelt = np.array(rainPlusSnowmelt)\n",
    "\n",
    "    # Runoff extraction\n",
    "    runOff = [[], [], [], [], []]\n",
    "\n",
    "    for i in range(5):\n",
    "        reach_c2_found = False  # Reset the flag for each file\n",
    "\n",
    "        with open(runOffFiles[i], 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "            for line in lines:\n",
    "                if 'C2' in line:\n",
    "                    reach_c2_found = True\n",
    "                    continue\n",
    "                if 'Runoff' in line:\n",
    "                    continue\n",
    "                            \n",
    "                columns = line.strip().split(',')\n",
    "                \n",
    "                if reach_c2_found and len(columns) == 2:\n",
    "                    runOff[i].append(float(columns[1].strip()))\n",
    "\n",
    "    runOff = np.array(runOff)\n",
    "\n",
    "    # HER calculation\n",
    "    HER = np.zeros([5, runOff.shape[1]])\n",
    "    percValues = [0.3097, 0.3643, 0.2372, 0.0397, 0.0491] #land use percentages for C2\n",
    "    cumRunOff = np.zeros([5, runOff.shape[1]])\n",
    "    cumRunOff[:, -1] = runOff[:, -1]\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in np.arange(runOff.shape[1] - 2, -1, -1):\n",
    "            cumRunOff[i, j] = cumRunOff[i, j + 1] + runOff[i, j] - HER[i, j + 1]\n",
    "            HER[i, j] = np.min([rainPlusSnowmelt[i, j], cumRunOff[i, j]])\n",
    "\n",
    "    HER_partial = np.zeros([5, runOff.shape[1]])\n",
    "    for i in range(5):\n",
    "        HER_partial[i, :] = percValues[i] * HER[i, :]\n",
    "\n",
    "    HER_final = np.sum(HER_partial, axis=0)\n",
    "    \n",
    "\n",
    "\n",
    "    # Change the corrected HER values (HER_final) into the HER_from_persist file\n",
    "    df = pd.read_csv(HER_from_persist, header= None, sep=None)  # Use header=None if the file doesn't have a header\n",
    "    # Update the second column (index 1) with HER_final values\n",
    "    df.iloc[:, 1] = HER_final  # Directly assign HER_final values to the second column\n",
    "    # Write the updated DataFrame back to a CSV file\n",
    "    df.to_csv(output_file, index=False, header=False, sep= ' ')  # Use index=False to avoid adding an extra index column\n",
    "\n",
    "    return HER_final\n",
    "\n",
    "\n",
    "#Main loop\n",
    "for member in range(0, 25):  # member from 0 to 24\n",
    "    for y in range(0, 24):    # year from 0 to 23\n",
    "        for m in range(0, 12):  # month from 0 to 11\n",
    "            # Define file paths for each iteration\n",
    "            snow_agr = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Agriculture.csv'\n",
    "            snow_broad_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Broad_leaved_Forest.csv'\n",
    "            snow_conf_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Coniferous_Forest.csv'\n",
    "            snow_small_veg = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Small_NoVegetation.csv'\n",
    "            snow_urb = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_snow_Urban.csv'\n",
    "\n",
    "            snowFiles = [snow_agr, snow_broad_forest, snow_conf_forest, snow_small_veg, snow_urb]\n",
    "\n",
    "            runoff_agr = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Agriculture.csv'\n",
    "            runoff_broad_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Broad_leaved_Forest.csv'\n",
    "            runoff_conf_forest = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Coniferous_Forest.csv'\n",
    "            runoff_small_veg = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Small_NoVegetation.csv'\n",
    "            runoff_urb = f'{path}/hindcast_persist_files/output_member{member}_year{y}_month{m}_runoff_Urban.csv'\n",
    "\n",
    "            runOffFiles = [runoff_agr, runoff_broad_forest, runoff_conf_forest, runoff_small_veg, runoff_urb]\n",
    "\n",
    "            HER_from_persist = f'{path}/hindcast_persist_files/cmd_member{member}_year{y}_month{m} C2.dat'\n",
    "            output_file = f'{path}/hindcast_inca_files/inca_C2_member{member}_year{y}_month{m}.dat'\n",
    "\n",
    "            # Call the HER calculation function with current file paths\n",
    "            HER_final = calculate_HER_final(snowFiles, runOffFiles, HER_from_persist, output_file)\n",
    "\n",
    "            # Optionally\n",
    "            #print(f'Processed member={member}, year={y}, month={m}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extracting SOC conditions from ERA5 (This is only required when SOC initial values forced with a new ERA5 dataset is required) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: SOC values were extracted for C1 and C2, but only Organic and Mineral SOC values from C1 were used as initial conditions of the system (after tests using C2 and C1-C2 average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Organic layer SOC C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path)  # Set the working directory\n",
    "\n",
    "# List of suffixes for the files to process\n",
    "file_suffixes = ['OL1', 'OL2', 'OL3', 'OL4', 'OL5', 'OL6']\n",
    "\n",
    "# Define the output file\n",
    "output_directory = f'{path}reanalysis/'\n",
    "output_file = f'{output_directory}Organic_SOC_C1_10year.txt'   #We used 10-year and 30-year runs of ERA5\n",
    "\n",
    "# Initialize a list to store the final data table\n",
    "final_data = []\n",
    "\n",
    "# Process each file\n",
    "for idx, suffix in enumerate(file_suffixes):\n",
    "    input_file = f'{path}reanalysis/inca_output_ERA5.{suffix}'\n",
    "    \n",
    "    # Try reading the file with different encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding) as file:\n",
    "                lines = file.readlines()\n",
    "            break  # If reading is successful, break out of the loop\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to decode {input_file} with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "    # Initialize variables for processing\n",
    "    reach_c1_found = False\n",
    "    file_data = []\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        # Check for the start of the relevant data block\n",
    "        if 'Sub-catchment 1 - 1' in line:\n",
    "            reach_c1_found = True\n",
    "            continue  # Skip this line and move to the next\n",
    "\n",
    "        # Break if the end of the relevant block is found\n",
    "        if 'Sub-catchment 2 - 2' in line:\n",
    "            break\n",
    "\n",
    "        # Process lines within the relevant block\n",
    "        if reach_c1_found:\n",
    "            # Split the line into columns\n",
    "            columns = line.split()\n",
    "\n",
    "            # Check if the line has the required number of columns\n",
    "            if len(columns) >= 7:\n",
    "                try:\n",
    "                    # Extract the date and SOC value\n",
    "                    date = columns[0]  # Date (first column)\n",
    "                    Fast_SOC = float(columns[2])  # Second column (convert to float)\n",
    "                    Slow_SOC = float(columns[6])  # Sixth column (convert to float)\n",
    "                    SOC_value = Fast_SOC + Slow_SOC  # Sum of the two SOC values\n",
    "                    \n",
    "                    # Ensure the date format is correct: ensure month has two digits\n",
    "                    day, month, year = date.split('/')\n",
    "                    month = month.zfill(2)  # Ensure month has 2 digits\n",
    "                    formatted_date = f'{day}/{month}/{year}'  # Reconstruct date with correct month format\n",
    "                    \n",
    "                    # Append to file_data\n",
    "                    file_data.append((formatted_date, SOC_value))\n",
    "                except ValueError:\n",
    "                    # Handle lines with non-numeric data\n",
    "                    print(f\"Skipping line in {input_file} due to non-numeric data: {line}\")\n",
    "\n",
    "    # If this is the first file, initialize final_data with dates\n",
    "    if idx == 0:\n",
    "        final_data = [[date] for date, _ in file_data]\n",
    "\n",
    "    # Add SOC values for the current suffix to final_data\n",
    "    for i, (_, soc_value) in enumerate(file_data):\n",
    "        final_data[i].append(soc_value)\n",
    "\n",
    "# Write the combined data to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    # Write the header\n",
    "    header = ['Date'] + file_suffixes\n",
    "    file.write('\\t'.join(header) + '\\n')\n",
    "\n",
    "    # Write each row of data\n",
    "    for row in final_data:\n",
    "        file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "print(f\"Combined data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Organic layer SOC C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path)  # Set the working directory\n",
    "\n",
    "# List of suffixes for the files to process\n",
    "file_suffixes = ['OL1', 'OL2', 'OL3', 'OL4', 'OL5', 'OL6']\n",
    "\n",
    "# Define the output file\n",
    "output_directory = f'{path}reanalysis/'\n",
    "output_file = f'{output_directory}Organic_SOC_C2.txt'\n",
    "\n",
    "# Initialize a list to store the final data table\n",
    "final_data = []\n",
    "\n",
    "# Process each file\n",
    "for idx, suffix in enumerate(file_suffixes):\n",
    "    input_file = f'{path}reanalysis/inca_output_ERA5.{suffix}'\n",
    "    \n",
    "    # Try reading the file with different encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding) as file:\n",
    "                lines = file.readlines()\n",
    "            break  # If reading is successful, break out of the loop\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to decode {input_file} with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "    # Initialize variables for processing\n",
    "    reach_c2_found = False\n",
    "    file_data = []\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        # Check for the start of the relevant data block\n",
    "        if 'Sub-catchment 2 - 2' in line:\n",
    "            reach_c2_found = True\n",
    "            continue  # Skip this line and move to the next\n",
    "\n",
    "        # Process lines within the relevant block\n",
    "        if reach_c2_found:\n",
    "            # Split the line into columns\n",
    "            columns = line.split()\n",
    "\n",
    "            # Check if the line has the required number of columns\n",
    "            if len(columns) >= 7:\n",
    "                try:\n",
    "                    # Extract the date and SOC value\n",
    "                    date = columns[0]  # Date (first column)\n",
    "                    Fast_SOC = float(columns[2])  # Second column (convert to float)\n",
    "                    Slow_SOC = float(columns[6])  # Sixth column (convert to float)\n",
    "                    SOC_value = Fast_SOC + Slow_SOC  # Sum of the two SOC values\n",
    "                    \n",
    "                    # Ensure the date format is correct: ensure month has two digits\n",
    "                    day, month, year = date.split('/')\n",
    "                    month = month.zfill(2)  # Ensure month has 2 digits\n",
    "                    formatted_date = f'{day}/{month}/{year}'  # Reconstruct date with correct month format\n",
    "                    \n",
    "                    # Append to file_data\n",
    "                    file_data.append((formatted_date, SOC_value))\n",
    "                except ValueError:\n",
    "                    # Handle lines with non-numeric data\n",
    "                    print(f\"Skipping line in {input_file} due to non-numeric data: {line}\")\n",
    "\n",
    "    # If this is the first file, initialize final_data with dates\n",
    "    if idx == 0:\n",
    "        final_data = [[date] for date, _ in file_data]\n",
    "\n",
    "    # Add SOC values for the current suffix to final_data\n",
    "    for i, (_, soc_value) in enumerate(file_data):\n",
    "        final_data[i].append(soc_value)\n",
    "\n",
    "# Write the combined data to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    # Write the header\n",
    "    header = ['Date'] + file_suffixes\n",
    "    file.write('\\t'.join(header) + '\\n')\n",
    "\n",
    "    # Write each row of data\n",
    "    for row in final_data:\n",
    "        file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "print(f\"Combined data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mineral layer SOC C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path)  # Set the working directory\n",
    "\n",
    "# List of suffixes for the files to process\n",
    "file_suffixes = ['ML1', 'ML2', 'ML3', 'ML4', 'ML5', 'ML6']\n",
    "\n",
    "# Define the output file\n",
    "output_directory = f'{path}reanalysis/'\n",
    "output_file = f'{output_directory}Mineral_SOC_C1.txt'\n",
    "\n",
    "# Initialize a list to store the final data table\n",
    "final_data = []\n",
    "\n",
    "# Process each file\n",
    "for idx, suffix in enumerate(file_suffixes):\n",
    "    input_file = f'{path}reanalysis/inca_output_ERA5.{suffix}'\n",
    "    \n",
    "    # Try reading the file with different encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding) as file:\n",
    "                lines = file.readlines()\n",
    "            break  # If reading is successful, break out of the loop\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to decode {input_file} with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "    # Initialize variables for processing\n",
    "    reach_c1_found = False\n",
    "    file_data = []\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        # Check for the start of the relevant data block\n",
    "        if 'Sub-catchment 1 - 1' in line:\n",
    "            reach_c1_found = True\n",
    "            continue  # Skip this line and move to the next\n",
    "\n",
    "        # Break if the end of the relevant block is found\n",
    "        if 'Sub-catchment 2 - 2' in line:\n",
    "            break\n",
    "\n",
    "        # Process lines within the relevant block\n",
    "        if reach_c1_found:\n",
    "            # Split the line into columns\n",
    "            columns = line.split()\n",
    "\n",
    "            # Check if the line has the required number of columns\n",
    "            if len(columns) >= 7:\n",
    "                try:\n",
    "                    # Extract the date and SOC value\n",
    "                    date = columns[0]  # Date (first column)\n",
    "                    Fast_SOC = float(columns[2])  # Second column (convert to float)\n",
    "                    Slow_SOC = float(columns[6])  # Sixth column (convert to float)\n",
    "                    SOC_value = Fast_SOC + Slow_SOC  # Sum of the two SOC values\n",
    "                    \n",
    "                    # Ensure the date format is correct: ensure month has two digits\n",
    "                    day, month, year = date.split('/')\n",
    "                    month = month.zfill(2)  # Ensure month has 2 digits\n",
    "                    formatted_date = f'{day}/{month}/{year}'  # Reconstruct date with correct month format\n",
    "                    \n",
    "                    # Append to file_data\n",
    "                    file_data.append((formatted_date, SOC_value))\n",
    "                except ValueError:\n",
    "                    # Handle lines with non-numeric data\n",
    "                    print(f\"Skipping line in {input_file} due to non-numeric data: {line}\")\n",
    "\n",
    "    # If this is the first file, initialize final_data with dates\n",
    "    if idx == 0:\n",
    "        final_data = [[date] for date, _ in file_data]\n",
    "\n",
    "    # Add SOC values for the current suffix to final_data\n",
    "    for i, (_, soc_value) in enumerate(file_data):\n",
    "        final_data[i].append(soc_value)\n",
    "\n",
    "# Write the combined data to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    # Write the header\n",
    "    header = ['Date'] + file_suffixes\n",
    "    file.write('\\t'.join(header) + '\\n')\n",
    "\n",
    "    # Write each row of data\n",
    "    for row in final_data:\n",
    "        file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "print(f\"Combined data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Mineral layer SOC C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path)  # Set the working directory\n",
    "\n",
    "# List of suffixes for the files to process\n",
    "file_suffixes = ['ML1', 'ML2', 'ML3', 'ML4', 'ML5', 'ML6']\n",
    "\n",
    "# Define the output file\n",
    "output_directory = f'{path}reanalysis/'\n",
    "output_file = f'{output_directory}Mineral_SOC_C2.txt'\n",
    "\n",
    "# Initialize a list to store the final data table\n",
    "final_data = []\n",
    "\n",
    "# Process each file\n",
    "for idx, suffix in enumerate(file_suffixes):\n",
    "    input_file = f'{path}reanalysis/inca_output_ERA5.{suffix}'\n",
    "    \n",
    "    # Try reading the file with different encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding) as file:\n",
    "                lines = file.readlines()\n",
    "            break  # If reading is successful, break out of the loop\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to decode {input_file} with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "    # Initialize variables for processing\n",
    "    reach_c1_found = False\n",
    "    file_data = []\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        # Check for the start of the relevant data block\n",
    "        if 'Sub-catchment 2 - 2' in line:\n",
    "            reach_c1_found = True\n",
    "            continue  # Skip this line and move to the next\n",
    "\n",
    "        # Process lines within the relevant block\n",
    "        if reach_c1_found:\n",
    "            # Split the line into columns\n",
    "            columns = line.split()\n",
    "\n",
    "            # Check if the line has the required number of columns\n",
    "            if len(columns) >= 7:\n",
    "                try:\n",
    "                    # Extract the date and SOC value\n",
    "                    date = columns[0]  # Date (first column)\n",
    "                    Fast_SOC = float(columns[2])  # Second column (convert to float)\n",
    "                    Slow_SOC = float(columns[6])  # Sixth column (convert to float)\n",
    "                    SOC_value = Fast_SOC + Slow_SOC  # Sum of the two SOC values\n",
    "                    \n",
    "                    # Ensure the date format is correct: ensure month has two digits\n",
    "                    day, month, year = date.split('/')\n",
    "                    month = month.zfill(2)  # Ensure month has 2 digits\n",
    "                    formatted_date = f'{day}/{month}/{year}'  # Reconstruct date with correct month format\n",
    "                    \n",
    "                    # Append to file_data\n",
    "                    file_data.append((formatted_date, SOC_value))\n",
    "                except ValueError:\n",
    "                    # Handle lines with non-numeric data\n",
    "                    print(f\"Skipping line in {input_file} due to non-numeric data: {line}\")\n",
    "\n",
    "    # If this is the first file, initialize final_data with dates\n",
    "    if idx == 0:\n",
    "        final_data = [[date] for date, _ in file_data]\n",
    "\n",
    "    # Add SOC values for the current suffix to final_data\n",
    "    for i, (_, soc_value) in enumerate(file_data):\n",
    "        final_data[i].append(soc_value)\n",
    "\n",
    "# Write the combined data to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    # Write the header\n",
    "    header = ['Date'] + file_suffixes\n",
    "    file.write('\\t'.join(header) + '\\n')\n",
    "\n",
    "    # Write each row of data\n",
    "    for row in final_data:\n",
    "        file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "print(f\"Combined data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run INCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Obtaining ouputs for subcatchment C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encodings to try\n",
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "\n",
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path)  # Set the working directory\n",
    "\n",
    "# Function to get the correct SOC values for a given date\n",
    "def get_soc_values(lines_soc, target_date):\n",
    "    for line in lines_soc:\n",
    "        # Check for matching date\n",
    "        if line.startswith(target_date):  # Match date in '1/MM/YYYY' format\n",
    "            return line.strip().split('\\t')[1:7]  # Extract columns 1–5\n",
    "    return ['NaN'] * 5  # Return NaNs if date not found\n",
    "\n",
    "# Loop through members, years, and months\n",
    "for member in range(0, 25):  # member from 0 to 24\n",
    "    for y in range(0, 24):  # year from 0 to 23\n",
    "        for m in range(0, 12):  # month from 0 to 11\n",
    "\n",
    "            # Calculate the target date for each iteration\n",
    "            month = 1 + m\n",
    "            year = 1993 + y\n",
    "            start_year = year - 5                     # IMPORTANT: Change warmup period if required!!!!!!!\n",
    "            target_date = f'1/{month:02d}/{start_year}'  # Ensure the month is two digits\n",
    "\n",
    "            # Log the date being processed\n",
    "            print(f\"Processing: {target_date}\")  # Verify the date is updated correctly\n",
    "            \n",
    "            # Update inca.dat files\n",
    "            shutil.copy(f'{path}/hindcast_inca_files/inca_C1_member{member}_year{y}_month{m}.dat', path + 'seasonal/hindcast_run_model/inca_C1.dat')\n",
    "            shutil.copy(f'{path}/hindcast_inca_files/inca_C2_member{member}_year{y}_month{m}.dat', path + 'seasonal/hindcast_run_model/inca_C2.dat')\n",
    "\n",
    "            # Read necessary files\n",
    "            with open(f'{path}seasonal/hindcast_bc_persist_input/hind_merged__member{member}_year{y}_month{m}.csv', 'r') as file:\n",
    "                lines_dat = file.readlines()  # Take Steps number\n",
    "                \n",
    "            with open(f'{path}reanalysis/Organic_SOC_C1.txt', 'r') as file:\n",
    "                lines_Org_SOC = file.readlines()\n",
    "                \n",
    "            with open(f'{path}reanalysis/Mineral_SOC_C1.txt', 'r') as file:\n",
    "                lines_Min_SOC = file.readlines()\n",
    "                \n",
    "            with open(f'{path}seasonal/hindcast_run_model/inca_par.par', 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Update Date and Steps in inca_par.par file\n",
    "            print(f\"Updating inca_par.par with date {target_date}\")\n",
    "            lines[15] = f'{target_date}\\n'  # Correctly update Date\n",
    "            lines[16] = lines_dat[0]  # Update Steps\n",
    "\n",
    "            # Update Organic_SOC and Mineral_SOC values based on the target date\n",
    "            org_soc_values = get_soc_values(lines_Org_SOC, target_date)\n",
    "            min_soc_values = get_soc_values(lines_Min_SOC, target_date)\n",
    "\n",
    "            # Log the SOC values to ensure they are correctly extracted\n",
    "            # print(f\"Organic SOC values for {target_date}: {org_soc_values}\")\n",
    "            # print(f\"Mineral SOC values for {target_date}: {min_soc_values}\")\n",
    "\n",
    "            # Update lines 8 and 11 with SOC values\n",
    "            lines[7] = ' '.join(org_soc_values) + '\\n'\n",
    "            lines[10] = ' '.join(min_soc_values) + '\\n'\n",
    "\n",
    "            # Write updated inca_par.par file\n",
    "            with open(f'{path}seasonal/hindcast_run_model/inca_par.par', 'w') as file:\n",
    "                file.writelines(lines)\n",
    "\n",
    "            print(lines[15])\n",
    "            print(lines[7])\n",
    "            print(lines[10])\n",
    "            print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "            # readFile = open(f'{path}seasonal/persist_inca/inca_par.par', 'r')    read the updated .par file to check\n",
    "\n",
    "            # line = readFile.readline()\n",
    "            # while line:\n",
    "                # print(line)\n",
    "                # line = readFile.readline()\n",
    "           # print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "       \n",
    "            # Run INCA simulation\n",
    "            os.chdir(path + 'seasonal/hindcast_run_model/')\n",
    "            os.system(\"inca_c_cmd_beta8.exe -par inca_par.par -dat inca_C1.dat inca_C2.dat -spatial inca_sts.sts\")\n",
    "\n",
    "            # time.sleep(2)   this adds time to run the model and then take the next run.\n",
    "\n",
    "            # Process INCA output\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(f'{path}seasonal/hindcast_run_model/inca_out.dsd', 'r', encoding=encoding) as file:\n",
    "                        lines = file.readlines()\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Failed to decode with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "            # Initialize variables for processing INCA output\n",
    "            reach_c2_found = False\n",
    "            data_lines = []\n",
    "\n",
    "            # Process each line in INCA output\n",
    "            for line in lines:\n",
    "                if 'Reach 2 - 2' in line:\n",
    "                    reach_c2_found = True\n",
    "                    continue\n",
    "                \n",
    "                if reach_c2_found:\n",
    "                    columns = line.split()\n",
    "                    if len(columns) >= 4:\n",
    "                        date = columns[0]\n",
    "                        flow = columns[1]\n",
    "                        open_water_doc = columns[4]\n",
    "                        data_lines.append((date, flow, open_water_doc))\n",
    "\n",
    "            # Skip the first three lines of data\n",
    "            data_lines = data_lines[3:]\n",
    "\n",
    "            # Save processed output\n",
    "            output_directory = f'{path}seasonal/hindcast_inca_output/'\n",
    "            output_file = f'{output_directory}inca_output_C2_member{member+1}_year{year}_month{month}.txt'\n",
    "\n",
    "            with open(output_file, 'w') as file:\n",
    "                for item in data_lines:\n",
    "                    file.write(f\"{item[0]}\\t{item[1]}\\t{item[2]}\\n\")\n",
    "\n",
    "            print(f\"Saved processed output for {target_date} to {output_file}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Otaining outputs for subcatchment C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encodings to try\n",
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "\n",
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path)  # Set the working directory\n",
    "\n",
    "# Function to get the correct SOC values for a given date\n",
    "def get_soc_values(lines_soc, target_date):\n",
    "    for line in lines_soc:\n",
    "        # Check for matching date\n",
    "        if line.startswith(target_date):  # Match date in '1/MM/YYYY' format\n",
    "            return line.strip().split('\\t')[1:7]  # Extract columns 1–5\n",
    "    return ['NaN'] * 5  # Return NaNs if date not found\n",
    "\n",
    "# Loop through members, years, and months\n",
    "for member in range(0, 25):  # member from 0 to 24\n",
    "    for y in range(0, 24):  # year from 0 to 23\n",
    "        for m in range(0, 12):  # month from 0 to 11\n",
    "\n",
    "            # Calculate the target date for each iteration\n",
    "            month = 1 + m\n",
    "            year = 1993 + y\n",
    "            start_year = year - 5\n",
    "            target_date = f'1/{month:02d}/{start_year}'  # Ensure the month is two digits\n",
    "\n",
    "            # Log the date being processed\n",
    "            print(f\"Processing: {target_date}\")  # Verify the date is updated correctly\n",
    "            \n",
    "            # Update inca.dat files\n",
    "            shutil.copy(f'{path}seasonal/hindcast_inca_files/inca_C1_member{member}_year{y}_month{m}.dat', path + 'seasonal/hindcast_run_model/inca_C1.dat')\n",
    "            shutil.copy(f'{path}seasonal/hindcast_inca_files/inca_C2_member{member}_year{y}_month{m}.dat', path + 'seasonal/hindcast_run_model/inca_C2.dat')\n",
    "        \n",
    "            # Read necessary files\n",
    "            with open(f'{path}seasonal/hindcast_bc_persist_input/hind_merged__member{member}_year{y}_month{m}.csv', 'r') as file:\n",
    "                lines_dat = file.readlines()  # Take Steps number\n",
    "                \n",
    "            with open(f'{path}reanalysis/Organic_SOC_C1.txt', 'r') as file:\n",
    "                lines_Org_SOC = file.readlines()\n",
    "                \n",
    "            with open(f'{path}reanalysis/Mineral_SOC_C1.txt', 'r') as file:\n",
    "                lines_Min_SOC = file.readlines()\n",
    "                \n",
    "            with open(f'{path}seasonal/hindcast_run_model/inca_par.par', 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Update Date and Steps in inca_par.par file\n",
    "            print(f\"Updating inca_par.par with date {target_date}\")\n",
    "            lines[15] = f'{target_date}\\n'  # Correctly update Date\n",
    "            lines[16] = lines_dat[0]  # Update Steps\n",
    "\n",
    "            # Update Organic_SOC and Mineral_SOC values based on the target date\n",
    "            org_soc_values = get_soc_values(lines_Org_SOC, target_date)\n",
    "            min_soc_values = get_soc_values(lines_Min_SOC, target_date)\n",
    "\n",
    "            # Log the SOC values to ensure they are correctly extracted\n",
    "            print(f\"Organic SOC values for {target_date}: {org_soc_values}\")\n",
    "            print(f\"Mineral SOC values for {target_date}: {min_soc_values}\")\n",
    "\n",
    "            # Update lines 8 and 11 with SOC values\n",
    "            lines[7] = ' '.join(org_soc_values) + '\\n'\n",
    "            lines[10] = ' '.join(min_soc_values) + '\\n'\n",
    "\n",
    "            # Write updated inca_par.par file\n",
    "            with open(f'{path}seasonal/hindcast_run_model/inca_par.par', 'w') as file:\n",
    "                file.writelines(lines)\n",
    "\n",
    "            print(lines[15])\n",
    "            print(lines[7])\n",
    "            print(lines[10])\n",
    "            print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "            #readFile = open(f'{path}seasonal/persist_inca/inca_par.par', 'r')\n",
    "\n",
    "            #line = readFile.readline()\n",
    "            #while line:\n",
    "                #print(line)\n",
    "                #line = readFile.readline()\n",
    "            #print(\"--------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "       \n",
    "            # Run INCA simulation\n",
    "            os.chdir(path + 'seasonal/hindcast_run_model/')\n",
    "            os.system(\"inca_c_cmd_beta8.exe -par inca_par.par -dat inca_C1.dat inca_C2.dat -spatial inca_sts.sts\")\n",
    "\n",
    "            #time.sleep(2)\n",
    "    \n",
    "    # Save INCA output files\n",
    "    #shutil.copy('inca_out.dsd', path + f'hindcast_output_bc/inca_out_member{member}_year{year}_month{month}.txt')\n",
    "    \n",
    "    #Process INCA output .dsd files\n",
    "    # Try opening the file with different encodings if UTF-8 fails\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(f'{path}hindcast_run_model/inca_out.dsd', 'r', encoding=encoding) as file:\n",
    "                        lines = file.readlines()\n",
    "                    break  # If reading is successful, break out of the loop\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Failed to decode with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "            # Initialize variables\n",
    "            reach_c1_found = False\n",
    "            data_lines = []\n",
    "\n",
    "            # Process each line\n",
    "            for line in lines:\n",
    "                if 'Reach 1 - 1' in line:\n",
    "                    reach_c1_found = True\n",
    "                    continue\n",
    "                \n",
    "                if 'Reach 2 -2' in line:\n",
    "                    break\n",
    "                \n",
    "                if reach_c1_found:\n",
    "                     # Split the line into columns\n",
    "                    columns = line.split()\n",
    "                    \n",
    "                    # Check if the line has the required number of columns\n",
    "                    if len(columns) >= 4:\n",
    "                    # Extract the Date (First column), Flow (2nd column) and Open water DOC (5th column)\n",
    "                        date = columns[0]\n",
    "                        flow = columns[1]\n",
    "                        open_water_doc = columns[4]\n",
    "                        data_lines.append((date, flow, open_water_doc))\n",
    "\n",
    "            # Skip the first three lines\n",
    "            data_lines = data_lines[3:]\n",
    "\n",
    "            # Define the output directory\n",
    "            output_directory = f'{path}hindcast_inca_output/'\n",
    "\n",
    "            # Save data_lines as a text file\n",
    "            output_file = f'{output_directory}inca_output_C1_member{member}_year{year}_month{month}.txt'\n",
    "            with open(output_file, 'w') as file:\n",
    "                for item in data_lines:\n",
    "                    file.write(f\"{item[0]}\\t{item[1]}\\t{item[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Processing INCA output from ERA5 (when necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/'\n",
    "os.chdir(path) #always run this line before runing a new test\n",
    "\n",
    "# Define the encodings to try\n",
    "encodings = ['latin1', 'iso-8859-1', 'utf-8']\n",
    "\n",
    "#Process INCA output .dsd files\n",
    "    # Try opening the file with different encodings if UTF-8 fails\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        with open(f'{path}reanalysis/inca_out.dsd', 'r', encoding=encoding) as file:  # inca_output_SEAS5_member0_year17_month2_warmup.dsd'\n",
    "            lines = file.readlines()\n",
    "        break  # If reading is successful, break out of the loop\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to decode with encoding {encoding}. Trying next encoding...\")\n",
    "\n",
    "# Initialize variables\n",
    "reach_c2_found = False\n",
    "data_lines = []\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    if 'Reach 2 - 2' in line:\n",
    "        \n",
    "        reach_c2_found = True\n",
    "        continue\n",
    "                \n",
    "    if reach_c2_found:\n",
    "            # Split the line into columns\n",
    "        columns = line.split()\n",
    "                    \n",
    "        # Check if the line has the required number of columns\n",
    "        if len(columns) >= 4:\n",
    "        # Extract the Date (First column), Flow (2nd column) and Open water DOC (5th column)\n",
    "            date = columns[0]\n",
    "            flow = columns[1]\n",
    "            open_water_doc = columns[4]\n",
    "            data_lines.append((date, flow, open_water_doc))\n",
    "\n",
    "# Skip the first three lines\n",
    "data_lines = data_lines[3:]\n",
    "\n",
    "# Define the output directory\n",
    "output_directory = f'{path}reanalysis/'\n",
    "\n",
    "# Save data_lines as a text file\n",
    "output_file = f'{output_directory}inca_out_C2_ERA5.txt'    #SEAS5_warmup\n",
    "with open(output_file, 'w') as file:\n",
    "    for item in data_lines:\n",
    "        file.write(f\"{item[0]}\\t{item[1]}\\t{item[2]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
