{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08321,
     "end_time": "2022-03-14T17:49:44.888825",
     "exception": false,
     "start_time": "2022-03-14T17:49:44.805615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![logo](./img/LogoLine_horizon_C3S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081751,
     "end_time": "2022-03-14T17:49:45.053733",
     "exception": false,
     "start_time": "2022-03-14T17:49:44.971982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082814,
     "end_time": "2022-03-14T17:49:45.21891",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.136096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merge Reanalysis (ERA5) and Hindcast (SEAS5) for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080985,
     "end_time": "2022-03-14T17:49:45.381362",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.300377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080595,
     "end_time": "2022-03-14T17:49:45.706112",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.625517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "The notebook has the following outline:\n",
    "* 1 - Join hindcast files (bias corrected): tp and t2m for each month, year and member\n",
    "* 2 - Merge ERA5 with hindcast files (ERA5: warm up period=5 years)\n",
    "* 3 - Combine datasets of subcatchments (C1 and C2) for modelling (PERSiST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081079,
     "end_time": "2022-03-14T17:49:46.683509",
     "exception": false,
     "start_time": "2022-03-14T17:49:46.60243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 2.519057,
     "end_time": "2022-03-14T17:52:09.394032",
     "exception": false,
     "start_time": "2022-03-14T17:52:06.874975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Miscellaneous operating system interfaces\n",
    "import os\n",
    "\n",
    "# Libraries for working with multi-dimensional arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# Libraries for plotting and geospatial data visualisation\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# To work with data labels in dictionary format\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Date and time related libraries\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from calendar import monthrange\n",
    "import datetime\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.274576,
     "end_time": "2022-03-14T17:52:10.724408",
     "exception": false,
     "start_time": "2022-03-14T17:52:10.449832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Join precipitation and temperature hindcast files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reach = 'C1' #reach1=C1, reach2=C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_bc/hindcast_bc_{reach}'\n",
    "output_path = f'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_bc_joined/hindcast_joined_{reach}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate temperature filenames\n",
    "def generate_temperature_filename(member, year, month):\n",
    "    return f\"hind_{reach}_t2m_member{member}_year{year}_month{month}.csv\"\n",
    "\n",
    "# Function to generate precipitation filenames\n",
    "def generate_precipitation_filename(member, year, month):\n",
    "    return f\"hind_{reach}_tp_member{member}_year{year}_month{month}.csv\"\n",
    "\n",
    "# Time range for precipitation and temperature files\n",
    "years = range(0, 24)  # Example range for 24 years (from 0 to 23 in the filenames)\n",
    "months = range(0, 12)  # Months from 0 to 11 (for January to December, 0 to 11)\n",
    "\n",
    "# Loop through each member\n",
    "for member in range(25):\n",
    "    # Loop through years and months to generate the filenames for both temperature and precipitation\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            # Generate filenames for temperature and precipitation\n",
    "            temperature_filename = generate_temperature_filename(member, year, month)\n",
    "            precipitation_filename = generate_precipitation_filename(member, year, month)\n",
    "            \n",
    "            # Construct full file paths\n",
    "            temperature_filepath = os.path.join(input_path, temperature_filename)\n",
    "            precipitation_filepath = os.path.join(input_path, precipitation_filename)\n",
    "            \n",
    "            # Check if files exist before processing\n",
    "            if not os.path.exists(temperature_filepath) or not os.path.exists(precipitation_filepath):\n",
    "                print(f\"Skipping non-existing file pair: {temperature_filename} and {precipitation_filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Read the temperature and precipitation CSV files\n",
    "            temperature_df = pd.read_csv(temperature_filepath, header=None, names=['Temperature'])\n",
    "            precipitation_df = pd.read_csv(precipitation_filepath, header=None, names=['Precipitation'])\n",
    "            \n",
    "            # Merge the data\n",
    "            merged_df = pd.concat([precipitation_df, temperature_df], axis=1)\n",
    "            \n",
    "            # Save the merged data to a new CSV file\n",
    "            output_filename = f\"hind_{reach}_joined_member{member}_year{year}_month{month}.csv\"\n",
    "            merged_df.to_csv(os.path.join(output_path, output_filename), sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Merge ERA5 with hindcast files (ERA5: warm up period= 1 or 5 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reach='C2' #Subcatchments, reach1=C1, reach2=C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path directories\n",
    "path_reanalysis = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/reanalysis'\n",
    "path_joined = f'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_bc_joined/hindcast_joined_{reach}'\n",
    "path_merged = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_bc_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "def extract_date(year_numeric, month_numeric):\n",
    "    #Extracts the first day of the actual 'joined' dataset period\n",
    "    year = 1993 + year_numeric\n",
    "    return pd.to_datetime(f'{year}-{month_numeric + 1}-01')\n",
    "\n",
    "def get_reanalysis_data_for_period(reanalysis_df, start_date):\n",
    "    #Extracts reanalysis data for the full year before start_date\n",
    "    start_date = pd.to_datetime(start_date)  \n",
    "    start_reanalysis = start_date - pd.DateOffset(years=5)  # IMPORTANT!!!! Change warmup period as required\n",
    "    end_reanalysis = start_date - pd.DateOffset(days=1)  \n",
    "\n",
    "    # Ensure 'time' column is in datetime format\n",
    "    reanalysis_df['time'] = pd.to_datetime(reanalysis_df['time'])\n",
    "\n",
    "    # Extract exactly 1 year of data\n",
    "    df_warmup_years = reanalysis_df[(reanalysis_df['time'] >= start_reanalysis) & \n",
    "                                    (reanalysis_df['time'] <= end_reanalysis)]\n",
    "\n",
    "    return df_warmup_years[['tp', 't2m']]  # Keep only tp and t2m columns\n",
    "\n",
    "# Load reanalysis data (with correct separator for whitespace)\n",
    "df_reanalysis = pd.read_csv(f'{path_reanalysis}/reanalysis_daily_{reach}_all_withDates.csv', sep=r'\\s+')\n",
    "\n",
    "# Convert time column to datetime format\n",
    "df_reanalysis['time'] = pd.to_datetime(df_reanalysis['time'])\n",
    "\n",
    "# Specify the ranges for member, year, and month\n",
    "members = range(25) \n",
    "years = range(24)  \n",
    "months = range(12)  \n",
    "\n",
    "for member in members:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            # Construct the filename for hindcast data\n",
    "            csv_file = f\"{path_joined}/hind_{reach}_joined_member{member}_year{year}_month{month}.csv\"\n",
    "            \n",
    "            # Check if the hindcast file exists\n",
    "            if os.path.exists(csv_file):\n",
    "                # Extract date using the function\n",
    "                date = extract_date(year, month)\n",
    "                \n",
    "                # Get reanalysis data for the warmup years before the extracted date\n",
    "                df_era5 = get_reanalysis_data_for_period(df_reanalysis, date)\n",
    "\n",
    "                # Load hindcast data and ensure correct data types for relevant columns\n",
    "                df_hind = pd.read_csv(csv_file, sep=r'\\s+', dtype={'Precipitation': float, 'Temperature': float})\n",
    "\n",
    "                # Check if 'Precipitation Temperature' column exists in hindcast data\n",
    "                if 'Precipitation Temperature' in df_hind.columns:\n",
    "                    df_hind[['tp', 't2m']] = df_hind['Precipitation Temperature'].str.split(expand=True)\n",
    "                    df_hind['tp'] = df_hind['tp'].astype(float)\n",
    "                    df_hind['t2m'] = df_hind['t2m'].astype(float)\n",
    "                    df_hind.drop(columns=['Precipitation Temperature'], inplace=True)\n",
    "                else:\n",
    "                    # If there are columns with different names, rename them to 'tp' and 't2m'\n",
    "                    if 'Precipitation' in df_hind.columns and 'Temperature' in df_hind.columns:\n",
    "                        df_hind = df_hind.rename(columns={'Precipitation': 'tp', 'Temperature': 't2m'})\n",
    "\n",
    "                # Ensure both DataFrames have exactly two columns: 'tp' and 't2m'\n",
    "                df_era5 = df_era5[['tp', 't2m']]  # Ensure the same two columns for era5\n",
    "                df_hind = df_hind[['tp', 't2m']]  # Ensure the same two columns for hindcast\n",
    "\n",
    "                # Concatenate df_era5 and df_hind vertically\n",
    "                df_merged = pd.concat([df_era5, df_hind], axis=0, ignore_index=True)\n",
    "\n",
    "                # Save the merged DataFrame to a new CSV file with no header and only the 'tp' and 't2m' columns\n",
    "                output_file = os.path.join(path_merged, f\"hind_merged_{reach}_member{member}_year{year}_month{month}.csv\")\n",
    "                df_merged.to_csv(output_file, index=False, header=False, sep=\" \")  # No header, no index, and using space separator\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Combine datasets of subcatchments (C1 and C2) for modelling (PERSiST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_bc_merged'\n",
    "output_path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_bc_persist_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files in the input directory\n",
    "csv_files = glob.glob(os.path.join(input_path, '*.csv'))\n",
    "\n",
    "# Create a dictionary to hold pairs of files\n",
    "file_pairs = {}\n",
    "\n",
    "# Identify and pair up files with 'C1' and 'C2' in the filename\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    if 'C1' in filename:\n",
    "        key = filename.replace('C1', '')\n",
    "        if key not in file_pairs:\n",
    "            file_pairs[key] = {}\n",
    "        file_pairs[key]['C1'] = file\n",
    "    elif 'C2' in filename:\n",
    "        key = filename.replace('C2', '')\n",
    "        if key not in file_pairs:\n",
    "            file_pairs[key] = {}\n",
    "        file_pairs[key]['C2'] = file\n",
    "\n",
    "# Process each pair of files\n",
    "for key, pair in file_pairs.items():\n",
    "    if 'C1' in pair and 'C2' in pair:\n",
    "        # Load the CSV files into pandas DataFrames\n",
    "        df1 = pd.read_csv(pair['C1'], header=None, names=['tp','t2m'])\n",
    "        df2 = pd.read_csv(pair['C2'], header=None, names=['tp','t2m'])\n",
    "\n",
    "        # Get the number of rows in df1\n",
    "        num_rows_df1 = len(df1)\n",
    "\n",
    "        # Create new rows without decimals\n",
    "        new_row_df1 = pd.DataFrame({'tp': [str(num_rows_df1)], 't2m': [None]})\n",
    "        new_row_2 = pd.DataFrame({'tp': [str(2)], 't2m': [None]})\n",
    "        new_row_3 = pd.DataFrame({'tp': ['C1'], 't2m': [None]})\n",
    "\n",
    "        # Add the rows in the correct order to df1\n",
    "        df1 = pd.concat([new_row_df1, new_row_2, new_row_3, df1]).reset_index(drop=True)\n",
    "\n",
    "        # Add the name 'C2' as a new row at index 0 in df2\n",
    "        new_row_4 = pd.DataFrame({'tp': ['C2'], 't2m': [None]})\n",
    "        df2 = pd.concat([new_row_4, df2], ignore_index=True)\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        result = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "        # Write the result to a new CSV file without column headers and with space separator\n",
    "        output_filename = f'{key}'\n",
    "        result.to_csv(os.path.join(output_path, output_filename), index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
